{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Brain Tractography Pipeline Tutorial\n",
    "\n",
    "This tutorial walks through a pipeline for generating tractography in fetal brains using diffusion tensor imaging (DTI). The pipeline covers several steps: from converting the tensor to an Orientation Distribution Function (ODF), to adjusting parcellations, creating a 5-tissue type (5TT) image, and generating tractography for further analysis.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. Convert the diffusion tensor image to an Orientation Distribution Function (ODF).\n",
    "2. Use a tissue segmentation image and a parcellation image to create necessary brain masks and adjust parcellations.\n",
    "3. Generate a 5-tissue type (5TT) image from the tissue segmentation.\n",
    "4. Adjust the parcellation to ensure white matter regions match the tissue segmentation.\n",
    "5. Separate gray and white matter regions for tractography extraction.\n",
    "6. Generate the gray matter-white matter interface (GMWMI) for tractography seeding.\n",
    "7. Use the whole brain mask to set tract length constraints.\n",
    "8. Generate tractography using the ODF, GMWMI, and the 5TT image.\n",
    "9. Convert the tractography to a format compatible with tract extraction.\n",
    "10. Extract specific tracts from the tractography.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- MRTrix (https://mrtrix.readthedocs.io/en/3.0.4/index.html)\n",
    "- tract_querier (https://tract-querier.readthedocs.io/en/latest/)\n",
    "    - Should be installed in in a python 2.7 environment\n",
    "\n",
    "## Python Packages\n",
    "- ants\n",
    "- DiPy\n",
    "- pandas\n",
    "- nibabel\n",
    "- numpy\n",
    "- SimpleITK\n",
    "- scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import subprocess\n",
    "\n",
    "#Get current working directory and file paths\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert Tensor to ODF\n",
    "\n",
    "In this step, we convert the diffusion tensor data into an ODF (Orientation Distribution Function). This step is necessary for generating tractography using the iFOD2 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2odf import compute_fod_sh\n",
    "\n",
    "# Load the diffusion tensor data\n",
    "tensor_path = os.path.join(current_dir, 'example', 'GA23.nii.gz')\n",
    "tensor = nib.load(tensor_path)\n",
    "tensor_data = tensor.get_fdata()\n",
    "\n",
    "# Compute the FOD (Fiber Orientation Distribution)\n",
    "fod_path = os.path.join(current_dir, 'example', 'GA23_odf.nii.gz')\n",
    "fod_data = compute_fod_sh(tensor_data)\n",
    "fod_img = nib.Nifti1Image(fod_data, tensor.affine)\n",
    "nib.save(fod_img, fod_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tissue Segmentation and Parcellation (Prerequisite)\n",
    "\n",
    "This step assumes you have already obtained a tissue segmentation image and a parcellation image. These segmentations are typically generated via atlas-based segmentation or a deep learning model (e.g., Davood). The segmentation is not included in this pipeline, but they are necessary inputs for the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a 5-Tissue Type (5TT) Image and Whole Brain Mask\n",
    "\n",
    "This step converts the tissue segmentation image into a 5TT image, which differentiates cortical gray matter, sub-cortical gray matter, white matter, cerebrospinal fluid (CSF), and pathological tissues. A whole brain mask is also created that can be used for tractography seeding. These step requires having a .csv file with the mapping of the different tissues. The label key for the CRL tissue segmentations can be found in 5tt_key.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fivett_gen import fivett_segmentation\n",
    "\n",
    "# Load the tissue segmentation data\n",
    "tissue_path = os.path.join(current_dir, 'example', 'GA23_tissue.nii.gz')\n",
    "md_path = os.path.join(current_dir, 'example', 'GA23_md.nii.gz')\n",
    "fivett_key_path = os.path.join(current_dir, '5tt_key.csv')\n",
    "\n",
    "tissue = nib.load(tissue_path)\n",
    "tissue_data = tissue.get_fdata()\n",
    "\n",
    "md = nib.load(md_path)\n",
    "md_data = md.get_fdata()\n",
    "\n",
    "# Generate 5TT image\n",
    "_5tt_img_path = os.path.join(current_dir, 'example', 'GA23_5tt.nii.gz')\n",
    "five_tt_img_data, whole_brain_data, csf_mask_data = fivett_segmentation(tissue_data, md_data, fivett_key_path, tissue.header.get_zooms()[0])\n",
    "five_tt_img = nib.Nifti1Image(five_tt_img_data, tissue.affine)\n",
    "nib.save(five_tt_img, _5tt_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Adjust Parcellation to Match the Tissue Segmentation\n",
    "\n",
    "This step ensures that the white matter regions in the parcellation imnage align perfectly with those in the tissue segmentation. Conflicting regions between the two segmentations are removed, and watershed dilation is applied to refine the boundaries. This step is necessary for accurate tract extraction after tractography generation. Please note that these will only work for the tissue segmentations of the CRL and the parcellation scheme based on the ENA55 Atlas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjust_parcellation import adjust_parcellation\n",
    "\n",
    "parcellation_path = os.path.join(current_dir, 'example', 'GA23_regional.nii.gz')\n",
    "parcellation_adjusted_path = os.path.join(current_dir, 'example', 'GA23_regional_adjusted.nii.gz')\n",
    "\n",
    "# Load the parcellation data\n",
    "parcellation = nib.load(parcellation_path)\n",
    "parcellation_data = parcellation.get_fdata()\n",
    "\n",
    "# Adjust the parcellation to align with tissue segmentation\n",
    "parcellation_adjusted_data = adjust_parcellation(tissue_data, parcellation_data)\n",
    "parcellation_adjusted_img = nib.Nifti1Image(parcellation_adjusted_data, parcellation.affine)\n",
    "nib.save(parcellation_adjusted_img, parcellation_adjusted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Separate Gray Matter and White Matter in Parcellation\n",
    "\n",
    "In this step, we divide the cortical parcellations into their respective gray and white matter regions. This ensures that the original ENA50 parcellations follow the FreeSurfer parcellation scheme. This step is crucial for the tract extraction step using tract_querier and the provided queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_ctx_wm import parcellation_ctx_wm\n",
    "\n",
    "voxel_size = tissue.header.get_zooms()[0]\n",
    "\n",
    "ctx_wm_parcellation_data = parcellation_adjusted_data.copy() # Should be used because the parcellation data is modified in the function\n",
    "\n",
    "# Create the cortical gray matter and white matter parcellation\n",
    "ctx_wm_parcellation_path = os.path.join(current_dir, 'example', 'GA23_ctx_wm.nii.gz')\n",
    "ctx_wm_parcellation_data = parcellation_ctx_wm(tissue_data, ctx_wm_parcellation_data, voxel_size)\n",
    "ctx_wm_parcellation_img = nib.Nifti1Image(ctx_wm_parcellation_data, tissue.affine)\n",
    "nib.save(ctx_wm_parcellation_img, ctx_wm_parcellation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Gray Matter-White Matter Interface (GMWMI)\n",
    "\n",
    "The 5TT image is used to generate the gray matter-white matter interface (GMWMI), which is used for seeding tractography. The GMWMI is the boundary between the gray and white matter regions. For the CRL data, using this type of seeding produces the best results. This image is calculated using the 5TT image and the command can be found in MRTrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5tt2gmwmi: \u001b[00;31m[WARNING] existing output files will be overwritten\u001b[0m\n",
      "5tt2gmwmi: [100%] uncompressing image \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_5tt.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\n",
      "5tt2gmwmi: [100%] Generating GMWMI seed mask\u001b[0K[0K\u001b[?7h\u001b[?7l\n",
      "5tt2gmwmi: [100%] compressing image \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_gmwmi.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['5tt2gmwmi', '/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_5tt.nii.gz', '/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_gmwmi.nii.gz', '-force'], returncode=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate GMWMI from 5TT image\n",
    "gmwmi_img_path = os.path.join(current_dir, 'example', 'GA23_gmwmi.nii.gz')\n",
    "_5tt_img_path = os.path.join(current_dir, 'example', 'GA23_5tt.nii.gz')\n",
    "\n",
    "command = ['5tt2gmwmi', _5tt_img_path, gmwmi_img_path, '-force']\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Minimum and Maximum Streamline Lengths\n",
    "\n",
    "To ensure reliable tractography, calculate the minimum and maximum lengths of the streamlines using the whole brain mask. This is important to filter out overly long streamlines (erroneous) or very short streamlines (U-fibers) that don't contribute to major tracts. If the tractography is to be used for structural connectivity, the last argument of the calculate ``calculate_streamline_lengths`` function should be changed to 'connectivity'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum streamline length: 24\n",
      "Maximum streamline length: 78\n"
     ]
    }
   ],
   "source": [
    "from get_max_min_streamline import calculate_streamline_lengths\n",
    "\n",
    "# Calculate streamline lengths\n",
    "streamline_lengths = calculate_streamline_lengths(whole_brain_data, tissue.header.get_zooms(), 'tract')\n",
    "minlength = streamline_lengths[0]\n",
    "maxlength = streamline_lengths[1]\n",
    "\n",
    "print(f'Minimum streamline length: {minlength}')\n",
    "print(f'Maximum streamline length: {maxlength}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Tractography\n",
    "\n",
    "We generate tractography using the ODF, GMWMI, and 5TT images with MRTrixâ€™s tckgen command. \n",
    "\n",
    "For crl fetal data, and when the tensor is converted to an ODF, the optimal parameters for the iFOD2 algorithm are:  -cutoff 0.001, -power 10, -angle 15, 20 or 25. For a list of the best angles for each tract, please refer to tract_angles.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tckgen: \u001b[00;31m[WARNING] existing output files will be overwritten\u001b[0m\n",
      "tckgen: [100%] uncompressing image \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_5tt.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\n",
      "tckgen: [100%] uncompressing image \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_gmwmi.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\n",
      "tckgen: [100%] uncompressing image \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_odf.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\n",
      "tckgen: [100%] preloading data for \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_odf.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\n",
      "tckgen: [100%] uncompressing image \"/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_5tt.nii.gz\"\u001b[0K[0K\u001b[?7h\u001b[?7l\n",
      "tckgen: [100%]    91809 seeds,    91809 streamlines,    10000 selected\u001b[0K\u001b[?7h\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['tckgen', '/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_odf.nii.gz', '/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23.tck', '-act', '/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_5tt.nii.gz', '-seed_gmwmi', '/Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_gmwmi.nii.gz', '-cutoff', '0.001', '-select', '10000', '-power', '10', '-angle', '15', '-minlength', '24', '-maxlength', '78', '-force'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf_image = os.path.join(current_dir, 'example', 'GA23_odf.nii.gz')\n",
    "tractography_output = os.path.join(current_dir, 'example', 'GA23.tck')\n",
    "\n",
    "act_image = _5tt_img_path\n",
    "seed_image = gmwmi_img_path\n",
    "\n",
    "# For CRL tensor, the threshold is 0.001, power of 10, angle of 15, 20 or 25\n",
    "streamlines_number = 10000\n",
    "power = 10\n",
    "angle = 15\n",
    "threshold = 0.001  \n",
    "\n",
    "command = ['tckgen', odf_image, tractography_output, '-act', act_image, '-seed_gmwmi', seed_image, '-cutoff', str(threshold), '-select', str(streamlines_number), '-power', str(power), '-angle', str(angle), '-minlength', str(minlength), '-maxlength', str(maxlength), '-force']\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Convert TCK to TRK Format\n",
    "\n",
    "Convert the tractography from .tck format to .trk format, which is required for tract extraction and visulization in TrackVis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.streamline import load_tractogram, save_tractogram\n",
    "\n",
    "def tck2trk(tck_file, anatomy_ref):\n",
    "    whole_brain = load_tractogram(tck_file, anatomy_ref)\n",
    "    trk_file = tck_file.replace(\".tck\", \".trk\")\n",
    "    save_tractogram(whole_brain, trk_file)\n",
    "    return trk_file\n",
    "\n",
    "trk_file_path = tck2trk(tractography_output, md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Extract Specific Tracts\n",
    "\n",
    "Use tract_querier from the WMQL to extract specific tracts from the tractography based on an anatomical query file. The query file \"crl_fetal_parcellation_ACT.qry\" and \"crl_fetal_parcellation_NON_ACT.qry\" can be used to extract tracts from segmentations based on the CRL tissue segmentations and the ENA55 atlas cortical parcellations. The main difference between the two query files is that the ACT query file is used for tractography generated using the Anatomically Constrained Tractography (ACT) algorithm, while the NON_ACT query file is used for tractography generated without ACT.\n",
    "\n",
    "Please note that steps 4 and 5 are crucial for this to work, and that if another another parcellation scheme is used, this query file would not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: tract_querier: command not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='tract_querier -t /Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23.trk -a /Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_ctx_wm.nii.gz -q /Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/GA23_ctx_wm.nii.gz -o /Users/camilocalixto/Dropbox/BCH/Jobs/Tractography_pipeline/example/tract_extraction/GA23', returncode=127)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tract_extraction_output = os.path.join(current_dir, 'example', 'tract_extraction', 'GA23')\n",
    "ctx_wm_parcellation_path = os.path.join(current_dir, 'example', 'GA23_ctx_wm.nii.gz')\n",
    "query_file = os.path.join(current_dir, 'crl_fetal_parcellation_ACT.qry')\n",
    "\n",
    "command = f\"tract_querier -t {trk_file_path} -a {ctx_wm_parcellation_path} -q {ctx_wm_parcellation_path} -o {tract_extraction_output}\"\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11 (optional): Extract Brain Structures\n",
    "\n",
    "If you want to extract brain structures from a segmentation/parcellation image, you can use the extract_individual_labels function. This function extracts individual labels from a segmentation image creating a binary mask for each label. This mask can then be used to visualize the brain structures in 3D or can be used as seeds for tractography. A .csv file with the mapping between the label values and the structure names is required. Here we use the labels.csv file to map the label values to the structure names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation file created for: Precentral_L\n",
      "Segmentation file created for: Precentral_R\n",
      "Segmentation file created for: Frontal_Sup_L\n",
      "Segmentation file created for: Frontal_Sup_R\n",
      "Segmentation file created for: Frontal_Sup_Orb_L\n",
      "Segmentation file created for: Frontal_Sup_Orb_R\n",
      "Segmentation file created for: Frontal_Mid_L\n",
      "Segmentation file created for: Frontal_Mid_R\n",
      "Segmentation file created for: Frontal_Mid_Orb_L\n",
      "Segmentation file created for: Frontal_Mid_Orb_R\n",
      "Segmentation file created for: Frontal_Inf_Oper_L\n",
      "Segmentation file created for: Frontal_Inf_Oper_R\n",
      "Segmentation file created for: Frontal_Inf_Tri_L\n",
      "Segmentation file created for: Frontal_Inf_Tri_R\n",
      "Segmentation file created for: Frontal_Inf_Orb_L\n",
      "Segmentation file created for: Frontal_Inf_Orb_R\n",
      "Segmentation file created for: Rolandic_Oper_L\n",
      "Segmentation file created for: Rolandic_Oper_R\n",
      "Segmentation file created for: Supp_Motor_Area_L\n",
      "Segmentation file created for: Supp_Motor_Area_R\n",
      "Segmentation file created for: Olfactory_L\n",
      "Segmentation file created for: Olfactory_R\n",
      "Segmentation file created for: Frontal_Sup_Medial_L\n",
      "Segmentation file created for: Frontal_Sup_Medial_R\n",
      "Segmentation file created for: Frontal_Med_Orb_L\n",
      "Segmentation file created for: Frontal_Med_Orb_R\n",
      "Segmentation file created for: Rectus_L\n",
      "Segmentation file created for: Rectus_R\n",
      "Segmentation file created for: Insula_L\n",
      "Segmentation file created for: Insula_R\n",
      "Segmentation file created for: Cingulum_Ant_L\n",
      "Segmentation file created for: Cingulum_Ant_R\n",
      "Segmentation file created for: Cingulum_Mid_L\n",
      "Segmentation file created for: Cingulum_Mid_R\n",
      "Segmentation file created for: Cingulum_Post_L\n",
      "Segmentation file created for: Cingulum_Post_R\n",
      "Segmentation file created for: ParaHippocampal_L\n",
      "Segmentation file created for: ParaHippocampal_R\n",
      "Segmentation file created for: Calcarine_L\n",
      "Segmentation file created for: Calcarine_R\n",
      "Segmentation file created for: Cuneus_L\n",
      "Segmentation file created for: Cuneus_R\n",
      "Segmentation file created for: Lingual_L\n",
      "Segmentation file created for: Lingual_R\n",
      "Segmentation file created for: Occipital_Sup_L\n",
      "Segmentation file created for: Occipital_Sup_R\n",
      "Segmentation file created for: Occipital_Mid_L\n",
      "Segmentation file created for: Occipital_Mid_R\n",
      "Segmentation file created for: Occipital_Inf_L\n",
      "Segmentation file created for: Occipital_Inf_R\n",
      "Segmentation file created for: Fusiform_L\n",
      "Segmentation file created for: Fusiform_R\n",
      "Segmentation file created for: Postcentral_L\n",
      "Segmentation file created for: Postcentral_R\n",
      "Segmentation file created for: Parietal_Sup_L\n",
      "Segmentation file created for: Parietal_Sup_R\n",
      "Segmentation file created for: Parietal_Inf_L\n",
      "Segmentation file created for: Parietal_Inf_R\n",
      "Segmentation file created for: SupraMarginal_L\n",
      "Segmentation file created for: SupraMarginal_R\n",
      "Segmentation file created for: Angular_L\n",
      "Segmentation file created for: Angular_R\n",
      "Segmentation file created for: Precuneus_L\n",
      "Segmentation file created for: Precuneus_R\n",
      "Segmentation file created for: Paracentral_Lobule_L\n",
      "Segmentation file created for: Paracentral_Lobule_R\n",
      "Segmentation file created for: Lentiform_L\n",
      "Segmentation file created for: Lentiform_R\n",
      "Segmentation file created for: Thalamus_L\n",
      "Segmentation file created for: Thalamus_R\n",
      "Segmentation file created for: Heschl_L\n",
      "Segmentation file created for: Heschl_R\n",
      "Segmentation file created for: Temporal_Sup_L\n",
      "Segmentation file created for: Temporal_Sup_R\n",
      "Segmentation file created for: Temporal_Pole_Sup_L\n",
      "Segmentation file created for: Temporal_Pole_Sup_R\n",
      "Segmentation file created for: Temporal_Mid_L\n",
      "Segmentation file created for: Temporal_Mid_R\n",
      "Segmentation file created for: Temporal_Pole_Mid_L\n",
      "Segmentation file created for: Temporal_Pole_Mid_R\n",
      "Segmentation file created for: Temporal_Inf_L\n",
      "Segmentation file created for: Temporal_Inf_R\n"
     ]
    }
   ],
   "source": [
    "from extract_labels import extract_individual_labels\n",
    "\n",
    "labelkey_path = os.path.join(current_dir, 'labelkey.csv')\n",
    "output_dir = os.path.join(current_dir, 'example', 'structure_extraction')\n",
    "\n",
    "extract_individual_labels(parcellation_adjusted_data, parcellation_adjusted_img.affine, \n",
    "labelkey_path, output_dir, base_name=\"GA23\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
